# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/4_preprocessing_volumes.ipynb.

# %% auto 0
__all__ = ['nifti_load', 'nifti_process', 'nifti_get_spacing', 'plot_2volumes', 'NormalizeSpacing', 'ResizeInterp',
           'PercentileCropIntensity', 'StandardizeIntensity', 'Dataset', 'DicomDataset', 'NiftiDataset',
           'DicomDataLoader', 'NiftiDataLoader']

# %% ../nbs/4_preprocessing_volumes.ipynb 3
from dicom_to_nifti import *

# %% ../nbs/4_preprocessing_volumes.ipynb 7
import nibabel
import torch

def nifti_load(series_base_path, serie_uid):
    nifti = nibabel.load(f"{series_base_path}/{serie_uid}.nii.gz")
    return nifti

def nifti_process(nifti):
    volume = torch.from_numpy(nifti.get_fdata()).to(torch.float32)
    metadata = nifti.header
    return volume, metadata 

def nifti_get_spacing(metadata):
    return torch.tensor(nifti.header.get_zooms())

# %% ../nbs/4_preprocessing_volumes.ipynb 15
import ipywidgets as widgets
import matplotlib.pyplot as plt
%matplotlib widget

def plot_volume(volume, fig, ax):

    volume = volume[0,0]

    plt.ioff()
    if fig is None or ax is None:
        fig, ax = plt.subplots(figsize=(5,5))

    ax_img = ax.imshow(volume[0])  # Create and cache plot element to modify

    slider = widgets.IntSlider(value=0, min=0, max=volume.shape[0]-1, step=1, description='Slice', readout=True, readout_format='d')

    def update(slice_num):
        ax_img.set_data(volume[slice_num])
        fig.canvas.draw_idle()

    slider.observe(lambda change: update(change["new"]), names="value")

    return widgets.VBox([fig.canvas, slider], layout=widgets.Layout(align_items="center"))

# %% ../nbs/4_preprocessing_volumes.ipynb 17
def plot_2volumes(volume1, volume2):

    ui1 = plot_volume(volume1, None, None)
    ui2 = plot_volume(volume2, None, None)
    return widgets.HBox([ui1, ui2])

# %% ../nbs/4_preprocessing_volumes.ipynb 19
import torch.nn.functional as F

class NormalizeSpacing:

    def __init__(self, interp_mode, domain_spacings_dict, get_metadata):
        self.interp_mode = interp_mode
        self.domain_spacings_dict = domain_spacings_dict
        self.get_metadata = get_metadata        

    def transform(self, volume, domain, spacing):

        assert volume.dtype == torch.float32, "Pixel array data has to be of type torch.float32."
        assert len(volume.shape) == 5, "Volume array must have 5 dimensions, i.e. (batch_size, channel_count, z, y, x)."
                
        domain_spacing = torch.tensor(self.domain_spacings_dict[domain])
        target_size = (torch.tensor(volume.shape[2:], dtype=torch.float32) / spacing * domain_spacing).to(torch.int32)
        volume = F.interpolate(volume, size=tuple(target_size), mode=self.interp_mode)
    
        return volume

# %% ../nbs/4_preprocessing_volumes.ipynb 24
class ResizeInterp:

    get_metadata = None
    
    def __init__(self, target_size, mode):
        self.target_size = target_size
        self.mode = mode    

    def transform(self, volume):
    
        assert volume.dtype == torch.float32, "Pixel array data has to be of type torch.float32"
        
        volume = F.interpolate(volume, size=self.target_size, mode=self.mode)
        return volume

# %% ../nbs/4_preprocessing_volumes.ipynb 28
import numpy as np

class PercentileCropIntensity:

    get_metadata = None

    def __init__(self, percentiles):
        self.percentiles = percentiles
    
    def transform(self, volume):
        percentiles = np.percentile(volume.flatten().detach().cpu().numpy(), self.percentiles)
        volume = torch.clamp(volume, min=percentiles[0], max=percentiles[1])
        return volume

# %% ../nbs/4_preprocessing_volumes.ipynb 32
class StandardizeIntensity:

    get_metadata = None
    
    def transform(self, volume):

        assert volume.dtype == torch.float32, \
        "Pixel array data has to be of type torch.float32"
    
        volume = (volume - volume.mean()) / volume.std()
        
        return volume

# %% ../nbs/4_preprocessing_volumes.ipynb 36
import torch

class Dataset(torch.utils.data.Dataset):

    def __init__(self, path, series_uid_l, labels, transforms):
        self.path = path
        self.series_uid_l = series_uid_l
        self.labels = labels
        self.transforms = transforms

        self.n = len(self.series_uid_l)
    
    def __len__(self):
        return self.n

# %% ../nbs/4_preprocessing_volumes.ipynb 37
import pydicom

class DicomDataset(Dataset):

    def __init__(self, path, series_uid_l, labels, transforms):
        super().__init__(path, series_uid_l, labels, transforms)

    def __getitem__(self, idx):

        serie_uid = self.series_uid_l[idx]
        label = self.labels[idx]

        ds_l = dicom_serie_load(self.path, serie_uid)
        volume, ds_metadata_l = dicom_serie_process(ds_l)
        volume = volume.unsqueeze(0).unsqueeze(0)

        for transform in self.transforms:
            if transform.get_metadata is not None:
                transform_metadata = transform.get_metadata(idx, ds_metadata_l)
                volume = transform.transform(volume, *transform_metadata)
            else:
                volume = transform.transform(volume)

        return volume[0], label

# %% ../nbs/4_preprocessing_volumes.ipynb 41
class NiftiDataset(Dataset):

    def __init__(self, path, series_uid_l, labels, transforms):
        super().__init__(path, series_uid_l, labels, transforms)

    def __getitem__(self, idx):

        serie_uid = self.series_uid_l[idx]
        label = self.labels[idx]

        nifti = nifti_load(self.path, serie_uid)
        volume, metadata = nifti_process(nifti)
        volume = volume.unsqueeze(0).unsqueeze(0)

        for transform in self.transforms:
            if transform.get_metadata is not None:
                transform_metadata = transform.get_metadata(idx, metadata)
                volume = transform.transform(volume, *transform_metadata)
            else:
                volume = transform.transform(volume)

        return volume[0], label

# %% ../nbs/4_preprocessing_volumes.ipynb 46
import multiprocessing

class DicomDataLoader:

    def __init__(self, path, series_uid_l, labels, unit_transforms,
                 shuffle, batch_size, pin_memory, num_workers):
        dataset = DicomDataset(path, series_uid_l, labels, unit_transforms)
        self.loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                                  shuffle=shuffle, pin_memory=pin_memory,
                                                  num_workers=num_workers)

    def __iter__(self):
        for volumes, labels in self.loader:
            yield volumes, labels

# %% ../nbs/4_preprocessing_volumes.ipynb 49
class NiftiDataLoader:

    def __init__(self, path, series_uid_l, labels, unit_transforms,
                 shuffle, batch_size, pin_memory, num_workers):
        dataset = NiftiDataset(path, series_uid_l, labels, unit_transforms)
        self.loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                                  shuffle=shuffle, pin_memory=pin_memory,
                                                  num_workers=num_workers)

    def __iter__(self):
        for volumes, labels in self.loader:
            yield volumes, labels
