{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOM to NifTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICOM is the international standard to store medical imaging information. It is flexible in that it allows to store all the data necessary for different acquisition protocols (modality, devices and workflows), which produce different metadata and image pixel data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medical imaging dataset can be subdivided in series of DICOM files, each series corresponding to a single patient's data. We can first write a function that loads one of these series into memory. In python, we can use the [pydicom](https://pydicom.github.io/pydicom/stable/) library to work with DICOM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dicom_to_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import pydicom\n",
    "\n",
    "def dicom_serie_load(series_base_path, serie_uid):\n",
    "\n",
    "    serie_path = f\"{series_base_path}/{serie_uid}\"\n",
    "    instances_filename = os.listdir(serie_path)\n",
    "    n_instances = len(instances_filename)\n",
    "\n",
    "    ds_l = [None] * n_instances\n",
    "    for i, instance_filename in enumerate(instances_filename):\n",
    "        ds_l[i] = pydicom.dcmread(f\"{serie_path}/{instance_filename}\")\n",
    "\n",
    "    return ds_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, '1.2.826.0.1.3680043.8.498.10124807242473374136099471315028464450')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path_dicom = f\"{os.environ[\"RSNA_IAD_DATA_DIR\"]}\"\n",
    "series_path_dicom = f\"{base_path_dicom}/series\"\n",
    "\n",
    "series_uid_l = os.listdir(series_path_dicom)\n",
    "ds_l = dicom_serie_load(series_path_dicom, series_uid_l[0])\n",
    "len(ds_l), ds_l[0].SOPInstanceUID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have list of `pydicom.Dataset` objects, corresponding to a (3D) volume. This volume can be contained in a single DICOM file (so the returned list of the function above will have only one element) or in multiple DICOM files, each one containing a single slice. A slice is a thin or cross-sectional image, created by scanning the body at a specific position along a chosen axis. Stacking these slices, according to that position, results in a volume. One simple way of doing this is obtaining the position of each of the slices along the z axis, sorting them by that position, and stacking them. We can obtain that position from the \"Image Position (Patient)\" attribute of the DICOM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_get_zposition(ds):\n",
    "    \n",
    "    if getattr(ds, \"ImagePositionPatient\", None) and len(ds.ImagePositionPatient) >= 3:\n",
    "        z_position = ds.ImagePositionPatient[2]\n",
    "    else:  # in case the tag is missing or does not contain the z axis position\n",
    "        z_position = ds.InstanceNumber\n",
    "\n",
    "    return float(z_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.935824681372, 32.460949707916)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_get_zposition(ds_l[0]), dicom_get_zposition(ds_l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: from now on i'm going to refer to data individual values as voxels (values in 3D space) instead of pixels (values in 2D space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the stored voxel values may not be in real-world units (like Hounsfield Units in CT imaging). So to convert those we need to use the \"Rescale Slope\" and \"Rescale Intercept\" attributes in the DICOM data to linearly transform the stored voxel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_get_rescale_factors(ds):\n",
    "\n",
    "    slope = getattr(ds, \"RescaleSlope\", 1.0)\n",
    "    intercept = getattr(ds, \"RescaleIntercept\", 0.0)\n",
    "    \n",
    "    return float(slope), float(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1.0, 0.0), (1.0, 0.0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_get_rescale_factors(ds_l[0]), dicom_get_rescale_factors(ds_l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of DICOM files is that they are slow to load from disk, because of all the flexibility to handle the different types of data, which makes parsing this data more computationally expensive. So they are not suitable for working with relatively large amounts of medical imaging data on a regular basis, like for deep learning experimentation. For that, a less flexible standard called NifTI is used. Each NifTI file has to contain a whole volume and may contain only a minimal set of metadata attributes to interpret and spatially place the volume's data, i.e.:\n",
    "    - the volume's shape.\n",
    "    - the space between voxels in real-world distance units.\n",
    "    - an affine transformation matrix, which maps the volumes voxel data in voxel coordinates to real-world coordinates.\n",
    "    - the scaling slope/intercept to linearly scale the voxel values.\n",
    "    - ...\n",
    "\n",
    "We could use the [dicom2nifti](https://github.com/icometrix/dicom2nifti) package to transform a whole series of DICOM files to a NifTI file automatically. However, since during inference we do want to load the DICOM files and go straight for the data transforms in predictions, and not have an intermediate step of saving them as NifTI files, we will write the functions to get the metadata we need from the DICOM, without the need of saving it as a NifTI file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the pixel/voxel array from the DICOM data (which is usually compressed) into an `torch.Tensor`, and we remove that data from a copy of the DICOM data (so we do not modify the original one). So we separate the array from the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "def dicom_split_array_from_metadata(ds):  # process a single DICOM\n",
    "\n",
    "    ds_copy = deepcopy(ds)  # copy the dicom object\n",
    "    pixel_pt = torch.from_numpy(ds_copy.pixel_array)\n",
    "    del ds_copy.PixelData  # remove pixel data from the dicom object copy\n",
    "    \n",
    "    return pixel_pt, ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512]) 65880\n"
     ]
    }
   ],
   "source": [
    "from objsize import get_deep_size\n",
    "slice_pt, ds = dicom_split_array_from_metadata(ds_l[0])\n",
    "print(slice_pt.shape, get_deep_size(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function that takes the list of `pydicom.Dataset` objects containing the DICOM data from a series and outputs a volume `torch.Tensor` and the `pydicom.Dataset` object without the pixel data, meaning, containing only the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_serie_process(ds_l):\n",
    "\n",
    "    n_ds = len(ds_l)\n",
    "\n",
    "    pixel_pt, ds_metadata = dicom_split_array_from_metadata(ds_l[0])  # Get first instance\n",
    "\n",
    "    if n_ds == 1:  # one dicom with the whole volume\n",
    "        volume = pixel_pt\n",
    "        ds_metadata_l = [ds_metadata]\n",
    "    else:  # each dicom with a slice\n",
    "        volume = torch.zeros((n_ds, *pixel_pt.shape), dtype=torch.float32)\n",
    "        ds_metadata_l = [None] * n_ds\n",
    "        volume[0] = pixel_pt\n",
    "        ds_metadata_l[0] = ds_metadata\n",
    "\n",
    "        # To later sort the slices\n",
    "        zpositions = torch.zeros((n_ds,), dtype=torch.float32)\n",
    "        zpositions[0] = dicom_get_zposition(ds_l[0])\n",
    "        \n",
    "        for i, ds in enumerate(ds_l[1:], start=1):\n",
    "            volume[i], ds_metadata_l[i] = dicom_split_array_from_metadata(ds)\n",
    "            zpositions[i] = dicom_get_zposition(ds)\n",
    "\n",
    "\t\t# sort slices in the volume\n",
    "        zpositions_argsort = torch.argsort(zpositions)\n",
    "        volume = volume[zpositions_argsort]\n",
    "\t\n",
    "    # rescale volume\n",
    "    slope, intercept = dicom_get_rescale_factors(ds_l[0])\n",
    "    volume = volume * slope + intercept\n",
    "    \n",
    "    return volume, ds_metadata_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([188, 512, 512]) 12156321\n"
     ]
    }
   ],
   "source": [
    "volume, ds_metadata_l = dicom_serie_process(ds_l)\n",
    "print(volume.shape, get_deep_size(ds_metadata_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the spacing between voxels to be similar, since the deep learning model that processes the volume will not capture that information. To normalize that spacing, we first need to obtain it from the DICOM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_get_spacing(ds):\n",
    "\n",
    "    pixel_spacing = getattr(ds, \"PixelSpacing\", None)\n",
    "    slice_thickness = getattr(ds, \"SliceThickness\", None)\n",
    "\n",
    "    if (pixel_spacing is None) or (slice_thickness is None):\n",
    "        shared_functional_groups_sequence = getattr(ds, \"SharedFunctionalGroupsSequence\", None)\n",
    "        if shared_functional_groups_sequence is not None:\n",
    "            pixel_measures_sequence = getattr(shared_functional_groups_sequence[0], \"PixelMeasuresSequence\", None)\n",
    "            if pixel_measures_sequence is not None:\n",
    "                if pixel_spacing is None:\n",
    "                    pixel_spacing = getattr(pixel_measures_sequence[0], \"PixelSpacing\", None)\n",
    "                if slice_thickness is None:\n",
    "                    slice_thickness = getattr(pixel_measures_sequence[0], \"SliceThickness\", None)\n",
    "    \n",
    "    if pixel_spacing is None or slice_thickness is None:\n",
    "        raise Exception(\"Missing either Pixel Spacing or Slice Thickness.\")\n",
    "    \n",
    "    pixel_spacing = [float(axis_spacing) for axis_spacing in pixel_spacing]\n",
    "    slice_thickness = float(slice_thickness)\n",
    "    spacing = [*pixel_spacing, slice_thickness]\n",
    "\n",
    "    return spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3515625, 0.3515625, 0.5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_get_spacing(ds_metadata_l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_serie_get_spacing(ds_l):\n",
    "\n",
    "    spacings = torch.zeros((len(ds_l), 3), requires_grad=False)\n",
    "    for i, ds in enumerate(ds_l):\n",
    "        spacings[i] = torch.tensor(dicom_get_spacing(ds))\n",
    "\n",
    "    return spacings.mode(dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3516, 0.3516, 0.5000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicom_serie_get_spacing(ds_metadata_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to store the data locally, we can use `nibabel` to save the volume (here we do not computed the affine transformation, since we won't use it), and the voxel spacing as a compressed NifTI file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import numpy as np\n",
    "import nibabel\n",
    "\n",
    "def dicom_volume_to_nifti(volume, ds_metadata_l, serie_uid, path):\n",
    "\n",
    "    nifti_path = f\"{path}/{serie_uid}.nii.gz\"\n",
    "\n",
    "    nifti = nibabel.nifti1.Nifti1Image(volume.detach().cpu().numpy(), affine=np.eye(4))\n",
    "    spacing = dicom_serie_get_spacing(ds_metadata_l)    \n",
    "    nifti.header.set_zooms(spacing)\n",
    "    \n",
    "    nibabel.save(nifti, nifti_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path_nifti = f\"{os.environ[\"RSNA_IAD_DATA_DIR\"]}/nifti\"\n",
    "series_path_nifti = f\"{base_path_nifti}/series\"\n",
    "if not os.path.exists(series_path_nifti):\n",
    "    os.mkdir(series_path_nifti)\n",
    "\n",
    "dicom_volume_to_nifti(volume, ds_metadata_l, series_uid_l[0], series_path_nifti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def dicom_serie_to_nifti(base_path_dicom, serie_uid, base_path_nifti):\n",
    "    ds_l = dicom_serie_load(base_path_dicom, serie_uid)\n",
    "    volume, ds_metadata_l = dicom_serie_process(ds_l)\n",
    "    dicom_volume_to_nifti(volume, ds_metadata_l,  serie_uid, base_path_nifti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_serie_to_nifti(series_path_dicom, series_uid_l[0], series_path_nifti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load and convert multiple DICOM series in parallel, we can use the `ProcessPoolExecutor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def dicom_series_to_niftis(base_path_dicoms, series_uid, base_path_niftis, max_workers):\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(dicom_serie_to_nifti, base_path_dicoms, serie_uid, base_path_niftis) for serie_uid in series_uid]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 5/5 [00:49<00:00,  9.99s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "samples_series_uid_l = random.sample(series_uid_l, 5)\n",
    "dicom_series_to_niftis(series_path_dicom, samples_series_uid_l, series_path_nifti, max_workers=5) # max_workers=multiprocessing.cpu_count() to use all cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this notebook, we cover the basics of how to load and process DICOM files data, and store that for analysis and experimentation, particularly, using the NifTI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_export(\"1_dicom_to_nifti.ipynb\", \"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
